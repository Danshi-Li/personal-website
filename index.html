<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Danshi Li</title>

    <meta name="author" content="Danshi Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Danshi Li
                </p>
                <p>I am a research engineer at Galbot, Led by <a href="https://hughw19.github.io">Prof. He Wang</a>, <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Dr. Zhizheng Zhang</a> and  <a href="https://ericyi.github.io">Prof. Li Yi</a>. 
			I obtained Master's degree from New York University and Bachelor's at CUHK. Previously I interned at GalBot and the EPIC Lab at Peking University, advised by <a href="https://hughw19.github.io">Prof. He Wang</a>
                </p>
                <p style="text-align:center">
                  <a href="danshi.li.academia@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a>CV</a> &nbsp;/&nbsp;
                  <a>Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=KJArKlgAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a>Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Danshi-Li">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Danshi-image.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Danshi-image.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am interested in learning generalizable, dexterous interactions between robot and the real world. Up to now, my works have been concentrated on the methodology of training in simulation and transferring to real world.
                </p>
		<p>
		  Recently I take great interest in the theory of <a href="https://arxiv.org/pdf/2008.02399"> geometric farics </a> and its applications in RL, e.g. in <a href="https://dextreme.org/fgp">dexterous hand manipulation</a> and <a href="https://sites.google.com/view/dextrah-g"> dexterous grasping </a>. Looking for interested partners to work together!	
		</p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>   

	

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src="images/DexGraspNet2.png" width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale Synthetic Cluttered Scenes</span>
        </a>
        <br>
              <a href="https://mzhmxzh.github.io">Jialiang Zhang*</a>,
	      <a href="https://lhrrhl0419.github.io">Haoran Liu*</a>,
	      <strong>Danshi Li*</strong>,
	      <a href="https://xinqiangyu.github.io/xinqiang-yu.github.io/index.html">Xinqiang Yu*</a>,
	      <a href="https://geng-haoran.github.io">Haoran Geng</a>,
	      <a href="https://selina2023.github.io">Yufei Ding</a>,
	      <a href="https://jychen18.github.io">Jiayi Chen</a>,
	      <a href="https://hughw19.github.io">He Wang</a>
	<br>
	      <em>CoRL 2024</em>
        <br>
	      <a href="https://pku-epic.github.io/DexGraspNet2.0/">website</a>
	      /
	      <a href="https://arxiv.org/pdf/2410.23004">arxiv</a>
	      /
	      <a href="https://openreview.net/pdf?id=5W0iZR9J7h">paper</a>
        <p></p>
        <p>
        We build a large-scale dataset of 429M dexterous grasping poses in 7500 cluttered scenes with benchmark simulation pipeline. Based on the abundance of data, we learn a generative dexterous grasp prediction model that efficiently leverage local geometric features. Our model achieves 90.7% success rate, and show strong robustness under downscaling of training dataset.
        </p>
      </td>
    </tr>

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src="images/STOPnet.jpg" width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects on Production Lines</span>
        </a>
        <br>
        <a href="https://yxkryptonite.github.io">Yuxuan Kuang*</a>,
        <a>Qin Han*</a>,
        <strong>Danshi Li</strong>,
	<a href="https://daiqy.github.io/">Qiyu Dai</a>,
        <a>Lian Ding</a>,
	<a>Dong Sun</a>,
	<a>Hanlin Zhao</a>,
	<a href="https://hughw19.github.io">He Wang</a>
	<br>
        <em>ICRA 2024</em>
        <br>
        <a href="https://arxiv.org/abs/2310.05717">arXiv</a>
        <p></p>
        <p>
        a framework for 6-DoF object suction grasp detection on production lines with generalizable NeRF reconstruction, with a focus on but not limited to transparent objects.
        </p>
      </td>
    </tr>
          </tbody></table>

        </td>
      </tr>
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
	  <tr>
	    <td>
	      <h2>Work Experience</h2>
	    </td>
	  </tr>
	</tbody></table>
	<tr style="padding:0px">
        	<td style="padding:0px">
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		          <tr>
		            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/PKU.png", width="90%"></td>
		            <td width="80%" valign="center">
		              <b>EPIC lab, Peking University</b>
		              <br> 2022.03 - Present 
		              <br><strong>Instructor: <a href="https://hughw19.github.io">Prof. He Wang</a></strong>
		            </td>
		          </tr>
			  <tr>
		            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/galbot.jpg", width="90%"></td>
		            <td width="80%" valign="center">
		              <b>Galbot</b>
		              <br> 2023.08 - 2024.07, Research Intern
			      <br> 2024.07 - Present, Research Engineer
		              <br><strong>Instructor: <a href="https://hughw19.github.io">Prof. He Wang</a>, <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Dr. Zhizheng Zhang</a></strong>
		            </td>
		          </tr>
		        </tbody></table>
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			  <tr>
			    <td>
			      <h2>Education</h2>
			    </td>
			  </tr>
			</tbody></table>
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		          <tr>
		            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/NYU.png", width="90%"></td>
		            <td width="80%" valign="center">
		              <b>New York University</b>
		              <br> 2022.09 - 2024.06
		              <br><strong>Master Student in Electrical and Computer Engineering</strong>
		            </td>
		          </tr>
			  <tr>
		            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/CUHK.png", width="90%"></td>
		            <td width="80%" valign="center">
		              <b>the Chinese University of Hong Kong</b>
		              <br> 2017.09 - 2022.07
		              <br><strong>Undergraduate Student in Mathematics</strong>
		            </td>
		          </tr>
		        </tbody></table>
		</td>
	</tr>
  </table>
  </body>
</html>
